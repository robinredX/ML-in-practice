{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "252a4206-b884-4fa8-bec5-07ec282d35cb",
   "metadata": {},
   "source": [
    "# Lab 2.2: Learning compact representations of single-cell data using AEs (Custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50857158-01f0-4e81-9108-c7d5ebc1fd9b",
   "metadata": {},
   "source": [
    "1. Implement the Autoencoder with the following configuration, save the model with a readable name. Use freytag data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba57adf9-5ad1-4150-b882-ce581fbd6051",
   "metadata": {
    "tags": []
   },
   "source": [
    "| Component | Layer dimension | \n",
    "| --- | --- | \n",
    "| Encoder | [256, 128, 64] | \n",
    "| Decoder | [128, 256, 512] | \n",
    "\n",
    "Bottleneck size: 16 \\\n",
    "Epochs: 100 \\\n",
    "Callbacks: A single callback monitoring validation loss with a patience of 5\\\n",
    "batch_size: 64 \\\n",
    "Optimizer: RMSProp (https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop)\n",
    "Learning rate = 1e-4 with a small weight decay (Decays the weights). <ins>Hint:</ins> It's a hyperparameter in RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a678cf3-7522-4805-bd30-b6786b9613f9",
   "metadata": {},
   "source": [
    "2. Train a classifier from yesterday (support vector classifier, random forest classifier or multilayered perceptron) on the bottleneck representations of the train set (Essentially, botteneck codes can be used as a data transformation)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
